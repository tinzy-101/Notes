<!DOCTYPE html>
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<title>Main_Memory</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<div id="9.1 Background"><h3 id="9.1 Background" class="header"><a href="#9.1 Background">9.1 Background</a></h3></div>
<pre>
	--&gt; for OS to exec programs, programs + data need to be partially in main mem during exec 
	--&gt; using CPU scheduling, can increase performance (but also need to share mem by putting lots of processes in mem) 
	--&gt; need to talk about how to manage mem (hardware alogs to complex software ones) 
		--&gt; depends mostly on hardware design
</pre>

<p>
<span id="9.1 Background-Background"></span><strong id="Background">Background</strong>
</p>
<pre>
	--&gt; mem = array of bytes each w/ address
	--&gt; instruction exec cycle
		1. CPU fetch instruction from mem based on program counter
		2. instruction decoded + maybe fetch operands from mem
		3. execute on operands
		4. return results to mem  
	--&gt; mem will only see stream of mem addresses (NOT how mem addresses generated + what they're for)
	--&gt; issues to deal with to manage mem
		--&gt; basic hardware, binding of virtual mem addresses to physics, logical vs physical addresses
</pre>
<p>
<span id="9.1 Background-Basic Hardware"></span><strong id="Basic Hardware">Basic Hardware</strong>
</p>
<pre>
	--&gt; main mem + registers in each core = only general purpose storage CPU can access directly 
		--&gt; data used in instructions need to be stored in one of these storages
	--&gt; register in core = 1 CPU cycle to access 
	--&gt; main mem (access thru bus) = many CPU cycles to access 
		--&gt; processor has to stall, but cache solves this problem 
		--&gt; cache managed thru hardware speeding up mem access (software doesn't do anything)
	--&gt; each process needs separate space in mem (protection) 
		--&gt; 2 registers:
			1. base register = smallest legal physical mem address
			2. limit register = size of range of pysical mem addresses 
				--&gt; e.g. base register = 300040, limit reg = 120900, then program can legally access addresses 300040-(30040+120900 -1) = 300040-420939
				--&gt; only kernel can load registers 
		--&gt; CPU hardware compares every mem address in user mod ew/ registers
			--&gt; if mismatch, user is out of bounds + fatal error 
			--&gt; user can change code/data structres of OS or other users 
	--&gt; kernal can access + modify user and OS mem, esp useful for context switches
</pre>
<p>
<span id="9.1 Background-Address Binding"></span><strong id="Address Binding">Address Binding</strong>
</p>
<pre>
	--&gt; program is on disk as binary executable file, needs to be brough into mem + put in context of process to run (when CPU is avail)
		--&gt; mem reclaimed from process when it stops exec
	--&gt; user process = any part of physical mem usually 
	--&gt; steps b4 user process can exec (some optional)
		1. compiler binds symbolic addreses of soure program to relocatable addresses (e.g. 14 bytes from beginning of this module)
		2. linker / loader binds relocatable addresses to absolute addresses (74014) 
	--&gt; binding instructions + data to mem addresses done at any step:
		1. compile time
			--&gt; if know where process will be in mem, then can generate absolute code
		2. load time 
			--&gt; if don't know where process will be during compile time, then compiler makes relocatable code
			--&gt; finish binding in load time 
		3. execution time 
			--&gt; if process can be moved to diff mem segment during execution time, can't bind til run time 
</pre>
<p>
<span id="9.1 Background-Logical vs Physical Address Space"></span><strong id="Logical vs Physical Address Space">Logical vs Physical Address Space</strong>
</p>
<pre>
	--&gt; logical address = gen by CPU
	--&gt; physical address = address loaded into mem-address register of mem 
	--&gt; binding addresses creates same logical/physical addresses, regardless of compile or load time
		--&gt; if exec time binding, then have diff logical + physical addresses
		--&gt; "virtual address" when binding during execution time
	--&gt; memory management unit (MMU) maps virtual to physical addresses during run time 
		--&gt; use base register as relocation register, add value in relocation register to every address generated by user proess when address sent to mem 
		--&gt; user program can't access physical addresses, only logical addresses 
		--&gt; process runs in memory locations from 0 to a max 
			--&gt; but, logical have to be mapped to physical before can be used 
</pre>
<p>
<span id="9.1 Background-Dynamic Loading"></span><strong id="Dynamic Loading">Dynamic Loading</strong>
</p>
<pre>
	--&gt; dynamic loading = routine not loaded until called 
		--&gt; when called, first check if routine already loaded
		--&gt; if not loaded, call relocatable linking loader to load + update program address tables
		--&gt; size of process not bound to size of physical mem 
		--&gt; user has to implement dynamic loading, OS provides libraries 
</pre>
<p>
<span id="9.1 Background-Dynamic Linking and shared libraries"></span><strong id="Dynamic Linking and shared libraries">Dynamic Linking and shared libraries</strong>
</p>
<pre>
	--&gt; dynamically linked libraries (DLLs) = sys libs linked to user programs when run program
		--&gt; static linking = sys libs treated like other object modules (combined into bin program image)
		--&gt; dynamic linking = linking at exec time (use w/ sys libs)
			--&gt; saves main mem + DLLs can be shared w/ ALL processes in mem 
	--&gt; program wants routine in dynamic lib, then linker loads DLL into mem, and DLL adjusts addresses in dynamic lib to location in mem where DLL stored. 
	--&gt; w/ dynamic linking, libraries are also updated dynamically = more convenient 
	--&gt; dynamic loading = don't need OS support, dynamic linking + shared libs = need OS support 
</pre>


<div id="9.2 Contiguous memory allocation"><h3 id="9.2 Contiguous memory allocation" class="header"><a href="#9.2 Contiguous memory allocation">9.2 Contiguous memory allocation</a></h3></div>
<pre>
	--&gt; main mem needs to accomodate OS + user processes 
		--&gt; need to efficiently allocate main mem 
		--&gt; early method = contiguous memory 
	contiguous mem:
		mem divided into OS and user partition 
		contiguous mem allocation = every process is together in mem 
</pre>
<p>
<span id="9.2 Contiguous memory allocation-Memory protection"></span><strong id="Memory protection">Memory protection</strong>
</p>
<pre>
	--&gt; relocation register (lowest possible mem address) + limit register (range of logical addresses)
	--&gt; CPU chooses process to exec, dispatcher loads relocation + limit registers 
		--&gt; every address en by CPU checked against these registers, so can protect OS and user programs from being modified by wrong person
	--&gt; using this scheme (relocation/limit) = can let OS size to change dynamically  
		--&gt; e.g. can move device drivers out of mem when not using it + load when need it
</pre>
<p>
<span id="9.2 Contiguous memory allocation-Memory allocation"></span><strong id="Memory allocation">Memory allocation</strong>
</p>
<pre>
	--&gt; simplest method: assign processes to partitions in mem 
		--&gt; OS keeps table of used/unused mem (hole) 
		--&gt; when new process comes in, need to allocate enough mem for it, find bi enough hole
	--&gt; dynamic storage allocation problem:
		--&gt; how to satisfy request of size "n" from lsit of holes 
		1. first fit:
				  1. allocate first available hole
		2. best fit:
				  1. allocate smallest hold that's big enough (search whole list)
		3. worse fit:
				  1. allocate largest hole (search whole list)
				  2. creates largest leftover hole 
		--&gt; first fit &amp; best fit &gt;&gt; worst fit
</pre>
<p>
<span id="9.2 Contiguous memory allocation-Fragmentation"></span><strong id="Fragmentation">Fragmentation</strong>
</p>
<pre>
	--&gt; external fragmentation = free mem space broken up when processes enter and leave mem 
		--&gt; first + best fit both have these problems
		--&gt; solution: compaction	
			--&gt; shuffle mem to form 1 big free hole (can only be done at exec time)
		--&gt; solution: paging
			--&gt; let logical addresses be noncontiguous (most commo)
	--&gt; internal fragmentation = give more mem than needed, wasted mem when process is running
</pre>


<div id="9.3 Paging"><h3 id="9.3 Paging" class="header"><a href="#9.3 Paging">9.3 Paging</a></h3></div>
<pre>
	--&gt; paging = mem management scheme that let's proccesses's physical address space be noncontiguous 
	--&gt; not external fragmentation + compaction (better than contiguous mem alloc)
	--&gt; OS and hardware need to work together
</pre>
<p>
<span id="9.3 Paging-Basic Method"></span><strong id="Basic Method">Basic Method</strong>
</p>
<pre>
	--&gt; physical mem into fixed blocks: frames
	--&gt; logical mem into same blocks: pages 
	--&gt; process exec: pages loaded into any avail mem from source (fs or backin store)
		--&gt; break bcking store into fixed blocks same size as mem frames  
	--&gt; address gen by CPU: page number + page offset
		--&gt; use page number as index for page table 
		--&gt; page table = base address of each frame in phys mem + offset for location in frame being referenced 
			--&gt; base frame address + page offset = phys mem address 
	
	1. CPU gen logical address, sent to MMU
	2. MMU split into page number + offset, put into page table
	3. page number = index for page table, entry in page table at index = number of frame of physical mem with the page 
	4. offset inside the page = same as offset inside frame
	5. physical address transalted from logical address now used to access physical mem, frame, and offset 
		
	--&gt; page size defined by hardware
		--&gt; usually 4kb-1gb per page, 2^n size 
		--&gt; if page number represented by 10 bits, there are 2^10 entries in the page table
	
	--&gt; logical address = page num + offset 
	
	--&gt; NO external frag w/ paging 
	
	1. process arrives in system to be executed 
	2. each page that the process needs = needs 1 frame
	3. if n frames avail, allocate them
	4. first page of process loaded onto allocated frames, then frame num put into apge table for process, next page onto next frame, etc. 
		--&gt; logical addresses translated to physical addresses (physical addresses NOT contiguous)
	
	--&gt; need frame table w/ entries for physical page frames (free or allocated)
</pre>
<p>
<span id="9.3 Paging-Hardware Support"></span><strong id="Hardware Support">Hardware Support</strong>
</p>
<pre>
	--&gt; need to load user registers + get correct hardware page table values from user page table when selecting process to exec 
	--&gt; implement page tables on hardware registers (take long time for context switch tho)
</pre>
<p>
<span id="9.3 Paging-Translation look-aside buffer"></span><strong id="Translation look-aside buffer">Translation look-aside buffer</strong>
</p>
<pre>
	--&gt; can implement page tables in hardware, but store them in main mem (faster context switch!)
		--&gt; but slower mem access times)
	--&gt; solution: use small and fast lookup hardware cache: translation look-aside buffer (TLB)
		--&gt; TLB has key + value, used to find values in page tables 
		--&gt; some values wired down (cant be removed) 
		--&gt; hit ratio = percentage of time page number is found 
		--&gt; need to implement paging based on hardware's TLB design 
</pre>
<p>
<span id="9.3 Paging-Protection"></span><strong id="Protection">Protection</strong>
</p>
<pre>
	--&gt; protect protection bits from each frame = protect entire paged environment 
	--&gt; each entry in page table also has valid-invalid bit
		--&gt; valid = page is in process's logical addess space 
</pre>
<p>
<span id="9.3 Paging-Shared Pages"></span><strong id="Shared Pages">Shared Pages</strong>
</p>
<pre>
	--&gt; reentrant code = can be shared (non self modifying, doesnt change during exec)
		--&gt; many processes can run code at same time, w/ own copy of registers + data stroage
</pre>


<div id="9.4 Structure of the page table"><h3 id="9.4 Structure of the page table" class="header"><a href="#9.4 Structure of the page table">9.4 Structure of the page table</a></h3></div>
<p>
<span id="9.4 Structure of the page table-Hierarchical paging"></span><strong id="Hierarchical paging">Hierarchical paging</strong>
</p>
<pre>
	--&gt; large logical address space for modern systems (2^32 or 2^64)
	--&gt; will have large page table that u want to divide into smaller pieces
		--&gt; e.g. page within a page, external page table is divided 
</pre>
<p>
<span id="9.4 Structure of the page table-Hashed page tables"></span><strong id="Hashed page tables">Hashed page tables</strong>
</p>
<pre>
	--&gt; hashed value = virtual page number
	--&gt; each entry in table has linked list of elements that hash to same location 
		--&gt; each element: virtual page num, value of mapped page frame, pointer to next element in list
	--&gt; basically, keep comparing virtual page nums to elements in list until find match 
</pre>
<p>
<span id="9.4 Structure of the page table-Inverted page tables"></span><strong id="Inverted page tables">Inverted page tables</strong>
</p>
<pre>
	--&gt; inverted page table = only 1 entry for each real page/frame of mem 
		--&gt; every entry = virtual address of page stored in real location 
	--&gt; overall: 1 page table for whole sys, one entry for every page in physical mem
	--&gt; addresses: process id + page number, search inverted page table for a match, and generate physical address
	--&gt; pro: less mem to store each table
	--&gt; con: more time to search table w/ a page reference 
		--&gt; use hash table to search for only a few page table entries 
</pre>


<div id="9.5 Swapping"><h3 id="9.5 Swapping" class="header"><a href="#9.5 Swapping">9.5 Swapping</a></h3></div>
<pre>
	--&gt; process / part of process can be swapped out of mem temporarily into a backing store when it's not being executing
	--&gt; with swapping: physical address space &gt; real physical mem of sys = more multiprogramming 
</pre>
<p>
<span id="9.5 Swapping-Standard Swapping"></span><strong id="Standard Swapping">Standard Swapping</strong>
</p>
<pre>
	--&gt; standard swapping = move whole processes between main mem and backing store (very fast secondary storage)
	--&gt; data structures associated with processes also need to be moved to backing store
	--&gt; pro: physical mem oversubsribed + accomodate more processes 
	--&gt; should be swapping out idle processes 
</pre>
<p>
<span id="9.5 Swapping-Swapping with paging"></span><strong id="Swapping with paging">Swapping with paging</strong>
</p>
<pre>
	--&gt; don't use standard swapping anymore b/c hard to move entire processes in/out of mem 
	--&gt; new strategy: only swap pages of a process 
</pre>
<div id="Explanation of Paging doc"><h3 id="Explanation of Paging doc" class="header"><a href="#Explanation of Paging doc">Explanation of Paging doc</a></h3></div>
<pre>
</pre>

</body>
</html>
